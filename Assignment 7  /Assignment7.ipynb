{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A7.2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "inOXWLgUaLCD"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn5z7B3JabjV"
      },
      "source": [
        "# data loading\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "# print(iris)\n",
        "iris_x = iris.data\n",
        "y = iris.target\n",
        "# print(y)\n",
        "\n",
        "print(iris_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GqnRwSsbvo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968b34b1-31d7-4756-b6af-e89ac2efa072"
      },
      "source": [
        "# Normalization. using MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "my_data = np.array(iris['data'])\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(my_data)\n",
        "nor_x = scaler.transform(my_data)\n",
        "\n",
        "print(nor_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.22222222 0.625      0.06779661 0.04166667]\n",
            " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
            " [0.11111111 0.5        0.05084746 0.04166667]\n",
            " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
            " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
            " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
            " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
            " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
            " [0.02777778 0.375      0.06779661 0.04166667]\n",
            " [0.16666667 0.45833333 0.08474576 0.        ]\n",
            " [0.30555556 0.70833333 0.08474576 0.04166667]\n",
            " [0.13888889 0.58333333 0.10169492 0.04166667]\n",
            " [0.13888889 0.41666667 0.06779661 0.        ]\n",
            " [0.         0.41666667 0.01694915 0.        ]\n",
            " [0.41666667 0.83333333 0.03389831 0.04166667]\n",
            " [0.38888889 1.         0.08474576 0.125     ]\n",
            " [0.30555556 0.79166667 0.05084746 0.125     ]\n",
            " [0.22222222 0.625      0.06779661 0.08333333]\n",
            " [0.38888889 0.75       0.11864407 0.08333333]\n",
            " [0.22222222 0.75       0.08474576 0.08333333]\n",
            " [0.30555556 0.58333333 0.11864407 0.04166667]\n",
            " [0.22222222 0.70833333 0.08474576 0.125     ]\n",
            " [0.08333333 0.66666667 0.         0.04166667]\n",
            " [0.22222222 0.54166667 0.11864407 0.16666667]\n",
            " [0.13888889 0.58333333 0.15254237 0.04166667]\n",
            " [0.19444444 0.41666667 0.10169492 0.04166667]\n",
            " [0.19444444 0.58333333 0.10169492 0.125     ]\n",
            " [0.25       0.625      0.08474576 0.04166667]\n",
            " [0.25       0.58333333 0.06779661 0.04166667]\n",
            " [0.11111111 0.5        0.10169492 0.04166667]\n",
            " [0.13888889 0.45833333 0.10169492 0.04166667]\n",
            " [0.30555556 0.58333333 0.08474576 0.125     ]\n",
            " [0.25       0.875      0.08474576 0.        ]\n",
            " [0.33333333 0.91666667 0.06779661 0.04166667]\n",
            " [0.16666667 0.45833333 0.08474576 0.04166667]\n",
            " [0.19444444 0.5        0.03389831 0.04166667]\n",
            " [0.33333333 0.625      0.05084746 0.04166667]\n",
            " [0.16666667 0.66666667 0.06779661 0.        ]\n",
            " [0.02777778 0.41666667 0.05084746 0.04166667]\n",
            " [0.22222222 0.58333333 0.08474576 0.04166667]\n",
            " [0.19444444 0.625      0.05084746 0.08333333]\n",
            " [0.05555556 0.125      0.05084746 0.08333333]\n",
            " [0.02777778 0.5        0.05084746 0.04166667]\n",
            " [0.19444444 0.625      0.10169492 0.20833333]\n",
            " [0.22222222 0.75       0.15254237 0.125     ]\n",
            " [0.13888889 0.41666667 0.06779661 0.08333333]\n",
            " [0.22222222 0.75       0.10169492 0.04166667]\n",
            " [0.08333333 0.5        0.06779661 0.04166667]\n",
            " [0.27777778 0.70833333 0.08474576 0.04166667]\n",
            " [0.19444444 0.54166667 0.06779661 0.04166667]\n",
            " [0.75       0.5        0.62711864 0.54166667]\n",
            " [0.58333333 0.5        0.59322034 0.58333333]\n",
            " [0.72222222 0.45833333 0.66101695 0.58333333]\n",
            " [0.33333333 0.125      0.50847458 0.5       ]\n",
            " [0.61111111 0.33333333 0.61016949 0.58333333]\n",
            " [0.38888889 0.33333333 0.59322034 0.5       ]\n",
            " [0.55555556 0.54166667 0.62711864 0.625     ]\n",
            " [0.16666667 0.16666667 0.38983051 0.375     ]\n",
            " [0.63888889 0.375      0.61016949 0.5       ]\n",
            " [0.25       0.29166667 0.49152542 0.54166667]\n",
            " [0.19444444 0.         0.42372881 0.375     ]\n",
            " [0.44444444 0.41666667 0.54237288 0.58333333]\n",
            " [0.47222222 0.08333333 0.50847458 0.375     ]\n",
            " [0.5        0.375      0.62711864 0.54166667]\n",
            " [0.36111111 0.375      0.44067797 0.5       ]\n",
            " [0.66666667 0.45833333 0.57627119 0.54166667]\n",
            " [0.36111111 0.41666667 0.59322034 0.58333333]\n",
            " [0.41666667 0.29166667 0.52542373 0.375     ]\n",
            " [0.52777778 0.08333333 0.59322034 0.58333333]\n",
            " [0.36111111 0.20833333 0.49152542 0.41666667]\n",
            " [0.44444444 0.5        0.6440678  0.70833333]\n",
            " [0.5        0.33333333 0.50847458 0.5       ]\n",
            " [0.55555556 0.20833333 0.66101695 0.58333333]\n",
            " [0.5        0.33333333 0.62711864 0.45833333]\n",
            " [0.58333333 0.375      0.55932203 0.5       ]\n",
            " [0.63888889 0.41666667 0.57627119 0.54166667]\n",
            " [0.69444444 0.33333333 0.6440678  0.54166667]\n",
            " [0.66666667 0.41666667 0.6779661  0.66666667]\n",
            " [0.47222222 0.375      0.59322034 0.58333333]\n",
            " [0.38888889 0.25       0.42372881 0.375     ]\n",
            " [0.33333333 0.16666667 0.47457627 0.41666667]\n",
            " [0.33333333 0.16666667 0.45762712 0.375     ]\n",
            " [0.41666667 0.29166667 0.49152542 0.45833333]\n",
            " [0.47222222 0.29166667 0.69491525 0.625     ]\n",
            " [0.30555556 0.41666667 0.59322034 0.58333333]\n",
            " [0.47222222 0.58333333 0.59322034 0.625     ]\n",
            " [0.66666667 0.45833333 0.62711864 0.58333333]\n",
            " [0.55555556 0.125      0.57627119 0.5       ]\n",
            " [0.36111111 0.41666667 0.52542373 0.5       ]\n",
            " [0.33333333 0.20833333 0.50847458 0.5       ]\n",
            " [0.33333333 0.25       0.57627119 0.45833333]\n",
            " [0.5        0.41666667 0.61016949 0.54166667]\n",
            " [0.41666667 0.25       0.50847458 0.45833333]\n",
            " [0.19444444 0.125      0.38983051 0.375     ]\n",
            " [0.36111111 0.29166667 0.54237288 0.5       ]\n",
            " [0.38888889 0.41666667 0.54237288 0.45833333]\n",
            " [0.38888889 0.375      0.54237288 0.5       ]\n",
            " [0.52777778 0.375      0.55932203 0.5       ]\n",
            " [0.22222222 0.20833333 0.33898305 0.41666667]\n",
            " [0.38888889 0.33333333 0.52542373 0.5       ]\n",
            " [0.55555556 0.54166667 0.84745763 1.        ]\n",
            " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
            " [0.77777778 0.41666667 0.83050847 0.83333333]\n",
            " [0.55555556 0.375      0.77966102 0.70833333]\n",
            " [0.61111111 0.41666667 0.81355932 0.875     ]\n",
            " [0.91666667 0.41666667 0.94915254 0.83333333]\n",
            " [0.16666667 0.20833333 0.59322034 0.66666667]\n",
            " [0.83333333 0.375      0.89830508 0.70833333]\n",
            " [0.66666667 0.20833333 0.81355932 0.70833333]\n",
            " [0.80555556 0.66666667 0.86440678 1.        ]\n",
            " [0.61111111 0.5        0.69491525 0.79166667]\n",
            " [0.58333333 0.29166667 0.72881356 0.75      ]\n",
            " [0.69444444 0.41666667 0.76271186 0.83333333]\n",
            " [0.38888889 0.20833333 0.6779661  0.79166667]\n",
            " [0.41666667 0.33333333 0.69491525 0.95833333]\n",
            " [0.58333333 0.5        0.72881356 0.91666667]\n",
            " [0.61111111 0.41666667 0.76271186 0.70833333]\n",
            " [0.94444444 0.75       0.96610169 0.875     ]\n",
            " [0.94444444 0.25       1.         0.91666667]\n",
            " [0.47222222 0.08333333 0.6779661  0.58333333]\n",
            " [0.72222222 0.5        0.79661017 0.91666667]\n",
            " [0.36111111 0.33333333 0.66101695 0.79166667]\n",
            " [0.94444444 0.33333333 0.96610169 0.79166667]\n",
            " [0.55555556 0.29166667 0.66101695 0.70833333]\n",
            " [0.66666667 0.54166667 0.79661017 0.83333333]\n",
            " [0.80555556 0.5        0.84745763 0.70833333]\n",
            " [0.52777778 0.33333333 0.6440678  0.70833333]\n",
            " [0.5        0.41666667 0.66101695 0.70833333]\n",
            " [0.58333333 0.33333333 0.77966102 0.83333333]\n",
            " [0.80555556 0.41666667 0.81355932 0.625     ]\n",
            " [0.86111111 0.33333333 0.86440678 0.75      ]\n",
            " [1.         0.75       0.91525424 0.79166667]\n",
            " [0.58333333 0.33333333 0.77966102 0.875     ]\n",
            " [0.55555556 0.33333333 0.69491525 0.58333333]\n",
            " [0.5        0.25       0.77966102 0.54166667]\n",
            " [0.94444444 0.41666667 0.86440678 0.91666667]\n",
            " [0.55555556 0.58333333 0.77966102 0.95833333]\n",
            " [0.58333333 0.45833333 0.76271186 0.70833333]\n",
            " [0.47222222 0.41666667 0.6440678  0.70833333]\n",
            " [0.72222222 0.45833333 0.74576271 0.83333333]\n",
            " [0.66666667 0.45833333 0.77966102 0.95833333]\n",
            " [0.72222222 0.45833333 0.69491525 0.91666667]\n",
            " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
            " [0.69444444 0.5        0.83050847 0.91666667]\n",
            " [0.66666667 0.54166667 0.79661017 1.        ]\n",
            " [0.66666667 0.41666667 0.71186441 0.91666667]\n",
            " [0.55555556 0.20833333 0.6779661  0.75      ]\n",
            " [0.61111111 0.41666667 0.71186441 0.79166667]\n",
            " [0.52777778 0.58333333 0.74576271 0.91666667]\n",
            " [0.44444444 0.41666667 0.69491525 0.70833333]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3TiFxFXtwtr"
      },
      "source": [
        "# =====>  Add intercept\n",
        "def add_intercept(x):\n",
        "    intr = np.ones((nor_x.shape[0], 1))\n",
        "    return np.concatenate((intr, nor_x), axis=1)\n",
        "# now data  is \n",
        "x = add_intercept(nor_x)\n",
        "y = np.array(iris['target'])\n",
        "# print(x[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS7iUmRmagz8",
        "outputId": "a48f08e4-f7b1-4e0a-e506-9c072e165058"
      },
      "source": [
        "# splinting data    train = 30%, test = 50%, validation = 20%\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_total = add_intercept(nor_x)\n",
        "y_total = y\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x_total,y_total,test_size = 0.5,random_state = 1)\n",
        "X_train,X_validation,y_train,y_validation = train_test_split(X_train,y_train,test_size = 0.4,random_state = 1)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_validation.shape)\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(45, 5)\n",
            "(30, 5)\n",
            "(75, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7-wmYit6Eig",
        "outputId": "e1960631-a5a6-4532-9d83-ece78cb4225d"
      },
      "source": [
        "# to check classes in dataset\n",
        "def num_Of_Classes(y):\n",
        "  l = [0 for i in range(0,100)]\n",
        "  for i in y:\n",
        "    l[i] = l[i]+1\n",
        "  count=0\n",
        "  classes=[]\n",
        "  for i in range(0,100):\n",
        "    if l[i]!=0:\n",
        "      count=count+1\n",
        "      classes.append(i)\n",
        "  return count,classes\n",
        "\n",
        "# check\n",
        "numberClasses,classes = num_Of_Classes(iris.target)\n",
        "print(numberClasses)\n",
        "print(classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "[0, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuvQV1Ku6W32",
        "outputId": "6fb21b71-b45b-4d54-8766-3e5fc2d21515"
      },
      "source": [
        "# set y for classifiers \n",
        "def onehotencoding(y_data,targetClass):\n",
        "  y =[]\n",
        "  for i in y_data:\n",
        "    if i == targetClass:\n",
        "      y.append(1)\n",
        "    else:\n",
        "      y.append(0)\n",
        "  return y\n",
        "\n",
        "# check \n",
        "t1 = onehotencoding(y_train,0)\n",
        "t2 = onehotencoding(y_train,1)\n",
        "t3 = onehotencoding(y_train,2)\n",
        "print(t1)\n",
        "print(t2)\n",
        "print(t3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1]\n",
            "[0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nWIl_h37YIW"
      },
      "source": [
        "def getYs(y,classes):\n",
        "  ys=[]\n",
        "  for x in classes:\n",
        "    ys.append(set_y(y,x))\n",
        "  return np.array(ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsJZWYZXak9u"
      },
      "source": [
        "# Sigmoid function to bound the hypothesis\n",
        "def sigmoid(z):                 \n",
        "    return 1/(1 + np.exp(-z))\n",
        "\n",
        "# log-loss function\n",
        "def loss(h, y):                \n",
        "    return (-y*np.log(h) - (1-y)*np.log(1-h)).mean()\n",
        "\n",
        "\n",
        "# mean square error\n",
        "def mse(h, y):                \n",
        "    return (((h-y)**2).mean())/2\n",
        "\n",
        "# predict probability\n",
        "def predict_prob(x):\n",
        "    return sigmoid(x)\n",
        "\n",
        "# \n",
        "def y_predict(x):\n",
        "    return predict_prob(x).round()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlHC10W6uUQD",
        "outputId": "c807749a-d57d-4a5c-eb67-efdc11cf358c"
      },
      "source": [
        "# logistic regression\n",
        "def train(x,y,epoch,alpha,rho):\n",
        "    weight = np.zeros(x.shape[1])\n",
        "    current_loss = 1  \n",
        "    # random weight initialization\n",
        "    for i in range(0, x.shape[1]):\n",
        "        weight[i] = random.uniform(-0.3, 0.3)\n",
        "    theta = weight \n",
        "    # print(weight)\n",
        "    for i in range(epoch):\n",
        "        # convergence if we reached the max epoch and\n",
        "        z = np.dot(x[i],theta)\n",
        "        # hypothesis\n",
        "        h = sigmoid(z)         \n",
        "        gradient = np.dot(x.T, (h - y)) / y.size \n",
        "\n",
        "        # new weights\n",
        "        theta = theta - alpha * gradient\n",
        "        z = np.dot(x, theta)\n",
        "        # y_predict\n",
        "        h = sigmoid(z)\n",
        "        y_pre = h\n",
        "        # print(h)\n",
        "        # log loss error\n",
        "        los = loss(h, y)  \n",
        "\n",
        "        # mse error     \n",
        "        mse_error = mse(h, y) \n",
        "        print(f'Epoch: {i} ------> ' + f'Log-Loss: {los} ' + f' ------> Mse: {mse_error} \\t ')\n",
        "        if(abs(current_loss - los) <= rho):\n",
        "            print(f\"Converged through roh criteria: epoch = {i}\")\n",
        "            break # converged\n",
        "        current_loss = los # save the previous error to calculate the diff in current error and previous error\n",
        "        if(i + 1 == epoch):\n",
        "            print(\"Converged through maximum epoch no criteria...\") # converged\n",
        "    return loss, mse_error\n",
        "\n",
        "train(X_train,y_train,5,1,0.001)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 ------> Log-Loss: 0.0891146863171922  ------> Mse: 0.30933411667200905 \t \n",
            "Epoch: 1 ------> Log-Loss: -0.12971323830726808  ------> Mse: 0.2865390870922515 \t \n",
            "Epoch: 2 ------> Log-Loss: -0.3158912498923429  ------> Mse: 0.2774649396714229 \t \n",
            "Epoch: 3 ------> Log-Loss: -0.4771813396740857  ------> Mse: 0.27436653326151866 \t \n",
            "Epoch: 4 ------> Log-Loss: -0.616164551849638  ------> Mse: 0.27230776657626543 \t \n",
            "Converged through maximum epoch no criteria...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<function __main__.loss>, 0.27230776657626543)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB9cxkVaulbL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}